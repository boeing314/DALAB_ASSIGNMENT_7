# DA5401 A7 â€“ Multiâ€‘Class Model Selection using ROC & PRC

## ðŸŽ¯ Objective
This project performs **multiâ€‘class landâ€‘cover classification** using the **UCI Landsat Satellite dataset** and evaluates multiple machine learning models using:
- **Weighted F1â€‘Score**
- **Macroâ€‘Averaged ROCâ€‘AUC**
- **Macroâ€‘Averaged Precisionâ€‘Recall (AP) Score**

The primary goal is to determine:
- The best model  
- The worst model  
- Effectiveness of evaluation metrics beyond simple accuracy  

---

## Dataset Summary
- **Dataset:** UCI Statlog (Landsat Satellite)
- **Classes:** 6 land cover types  
- **Features:** Spectral reflectance data from satellite sensors  
- **Challenge:** Moderate class overlap and imbalance

Source:  
https://archive.ics.uci.edu/dataset/146/statlog+landsat+satellite

---

## ðŸ§ª Models Trained
| Model | Library | Notes |
|-------|---------|------|
| KNN (k=5) | sklearn | Instanceâ€‘based baseline |
| Decision Tree | sklearn | High variance |
| Logistic Regression | sklearn | Linear decision boundaries |
| Gaussian Naive Bayes | sklearn | Independence assumption often invalid |
| Dummy Classifier (Prior) | sklearn | Baseline reference |
| SVC (probability=True) | sklearn | Nonâ€‘linear kernel â†’ expected best performance |
| **Brownie Points** ðŸš€ |  |  |
| Random Forest | sklearn | Ensemble of decision trees |
| XGBoost | xgboost | Gradient boosting, strong model |
| **KNNâ€‘1 (Veryâ€‘Poor Model)** | sklearn | Overfits â†’ worst Macro AUC |

---

## âš™ï¸ Methodology

###  Part A â€“ Baseline Metrics
- Standardization using `StandardScaler`
- Train/Test split: **85% / 15%**
- Compute **Weighted F1**

###  Part B â€“ ROC Analysis
- OvR (Oneâ€‘vsâ€‘Rest) Multiâ€‘Class ROC
- Macroâ€‘averaged AUC
- Single ROC plot with all model curves

###  Part C â€“ PRC Analysis
- Multiâ€‘Class Macroâ€‘Averaged Precisionâ€‘Recall curves
- Compute **Average Precision (AP)**

###  Part D â€“ Final Model Selection
Comparison of Rankings (Highest â†’ Lowest):

**Weighted F1 Ranking**
> KNN > SVC > Decision Tree > Logistic Regression > GaussianNB > Dummy Prior

**Macro ROCâ€‘AUC Ranking**
> SVC > KNN > Logistic Regression > GaussianNB > Decision Tree > Dummy Prior

**Macro AP Ranking**
> SVC > KNN > Logistic Regression > GaussianNB > Decision Tree > Dummy Prior

- **SVC consistently top in thresholdâ€‘based metrics**  
- Dummy Prior performs like random guessing (AUC â‰ˆ 0.50)  
- **KNNâ€‘1** (Brownie Model) â†’ AUC < 0.50 due to noise sensitivity

---

## Final Recommendation

> **Support Vector Classifier (SVC)** is the best model for this classification task

---

##  Key Plots Included in Notebook
- Macroâ€‘Averaged OvR ROC Curve (All Models)
- Macroâ€‘Averaged Precisionâ€‘Recall Curve (All Models)
- Performance Summary Table (F1, AUC, AP)

---

##  Files Included
| File | Description |
|------|------------|
| A7.ipynb | Full implementation with visualizations and analysis |
| README.md | This summary report |

---


## ðŸ”– Citation
Blake, C. & Merz, C.J. (1998). UCI Repository of machine learning databases. University of California, Irvine.

---

Thank you! ðŸ˜Š  
