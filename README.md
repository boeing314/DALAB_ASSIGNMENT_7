# DA5401 A7 â€“ Multiâ€‘Class Model Selection using ROC & PRC

## ğŸ“Œ Course
DA5401 â€“ Machine Learning  
Assignment 7: Model Selection and Performance Analysis  
Institution: [Your University/Department]  
Submitted by: **[Your Name]**  
Roll No: **[Your Roll Number]**

---

## ğŸ¯ Objective
This project performs **multiâ€‘class landâ€‘cover classification** using the **UCI Landsat Satellite dataset** and evaluates multiple machine learning models using:
- **Weighted F1â€‘Score**
- **Macroâ€‘Averaged ROCâ€‘AUC**
- **Macroâ€‘Averaged Precisionâ€‘Recall (AP) Score**

The primary goal is to determine:
âœ… The best model  
âœ… The worst model  
âœ… Effectiveness of evaluation metrics beyond simple accuracy  

---

## ğŸ“‚ Dataset Summary
- **Dataset:** UCI Statlog (Landsat Satellite)
- **Classes:** 6 land cover types  
- **Features:** Spectral reflectance data from satellite sensors  
- **Challenge:** Moderate class overlap and imbalance

Source:  
https://archive.ics.uci.edu/dataset/146/statlog+landsat+satellite

---

## ğŸ§ª Models Trained
| Model | Library | Notes |
|-------|---------|------|
| KNN (k=5) | sklearn | Instanceâ€‘based baseline |
| Decision Tree | sklearn | High variance |
| Logistic Regression | sklearn | Linear decision boundaries |
| Gaussian Naive Bayes | sklearn | Independence assumption often invalid |
| Dummy Classifier (Prior) | sklearn | Baseline reference |
| SVC (probability=True) | sklearn | Nonâ€‘linear kernel â†’ expected best performance |
| **Brownie Points** ğŸš€ |  |  |
| Random Forest | sklearn | Ensemble of decision trees |
| XGBoost | xgboost | Gradient boosting, strong model |
| **KNNâ€‘1 (Veryâ€‘Poor Model)** | sklearn | Overfits â†’ worst Macro AUC |

---

## âš™ï¸ Methodology

### âœ”ï¸ Part A â€“ Baseline Metrics
- Standardization using `StandardScaler`
- Train/Test split: **85% / 15%**
- Compute **Weighted F1**

### âœ”ï¸ Part B â€“ ROC Analysis
- OvR (Oneâ€‘vsâ€‘Rest) Multiâ€‘Class ROC
- Macroâ€‘averaged AUC
- Single ROC plot with all model curves

### âœ”ï¸ Part C â€“ PRC Analysis
- Multiâ€‘Class Macroâ€‘Averaged Precisionâ€‘Recall curves
- Compute **Average Precision (AP)**

### âœ”ï¸ Part D â€“ Final Model Selection
Comparison of Rankings (Highest â†’ Lowest):

**Weighted F1 Ranking**
> KNN > SVC > Decision Tree > Logistic Regression > GaussianNB > Dummy Prior

**Macro ROCâ€‘AUC Ranking**
> SVC > KNN > Logistic Regression > GaussianNB > Decision Tree > Dummy Prior

**Macro AP Ranking**
> SVC > KNN > Logistic Regression > GaussianNB > Decision Tree > Dummy Prior

âœ… **SVC consistently top in thresholdâ€‘based metrics**  
âœ… Dummy Prior performs like random guessing (AUC â‰ˆ 0.50)  
âœ… **KNNâ€‘1** (Brownie Model) â†’ AUC < 0.50 due to noise sensitivity

---

## ğŸ† Final Recommendation

> **Support Vector Classifier (SVC)** is the best model for this classification task

Reasons:
- Highest ROCâ€‘AUC and AP â†’ strong threshold performance
- Nonâ€‘linear boundary fits Landsat spectral complexity
- Robust against overlapping classes

ğŸ“‰ Worst Model:
> **KNNâ€‘1** â†’ severe overfitting â†’ performs worse than random

---

## ğŸ“Š Key Plots Included in Notebook
- Macroâ€‘Averaged OvR ROC Curve (All Models)
- Macroâ€‘Averaged Precisionâ€‘Recall Curve (All Models)
- Performance Summary Table (F1, AUC, AP)

---

## âœ… Files Included
| File | Description |
|------|------------|
| A7.ipynb | Full implementation with visualizations and analysis |
| README.md | This summary report |

---

## âœï¸ Authorâ€™s Notes
This assignment demonstrates why **thresholdâ€‘independent** metrics (AUC, AP) provide a more **trustworthy** measure of multiâ€‘class model performance, especially when dataset imbalance exists.

---

## ğŸ”– Citation
Blake, C. & Merz, C.J. (1998). UCI Repository of machine learning databases. University of California, Irvine.

---

Thank you! ğŸ˜Š  
